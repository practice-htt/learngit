\documentclass[a4paper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{ctex}
%\usepackage{authblk}
\usepackage{abstract}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{appendix}
\usepackage{floatrow}
\usepackage{cite}
\usepackage{enumerate}
\floatsetup[table]{capposition=top}
\newfloatcommand{capbtabbox}{table}[][\FBwidth]
%并排插入两张图像
\usepackage{graphicx} %use graph format
%\usepackage{caption}
\usepackage{subfigure}

\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\floatname{algorithm}{算法}
\renewcommand{\algorithmicrequire}{\textbf{输入:}}
\renewcommand{\algorithmicensure}{\textbf{输出:}}

\renewcommand{\abstractname}{摘要}
\newtheorem{theorem}{定理}[section]
\newtheorem{example}{Example}[section]


\title{语音合成综述}
\author{黄鹏斐}
%\author[1]{yang}
%\author[1]{huang}
%\author[1]{liu}
%\affil[1]{nku}
\date{}

\begin{document}
\maketitle

\begin{abstract}
\noindent\textbf{关键词：}
\end{abstract}

\section{语音合成发展历史}
语音合成是指通过人工的方法合成语音，使计算机像人一样能够发出清晰、自然的声音，根据人从准备说话到最终产生语音的过程，可分为三个层次：从意向到语音（Intention to Speech），从概念到语音（Concept to Speech），从文字到语音（Text to Speech）\cite{ref1}。一般而言我们研究的是从文字到语音的转换，简称为TTS。TTS 已经有着两百多年的历史，最早在1778 年，法国人Mical制作了一个能说话的装置，到1873 年，出现了一台模拟人发音生理过程的机械式仪器。

1937年，Duley.H等人沿着模拟人类发音的物理机制的思路，将语音产生抽象成源-滤波器模型，用电子线路来产生声音，研制了语音合成器VODER\cite{ref2}。其中，在声源用噪声模拟清音，周期脉冲模拟浊音，通过电子通道滤波器模拟声道的滤波作用。基于这一思路，出现了到现在仍十分有效的参数合成法，其中最具代表性的是共振峰合成法和线性预测编码方法。理想中，通过调节共振峰参数，能得到多变的合成语音，但是准确提取参数是非常困难复杂的，因此实际上合成的语音质量并不能满足应用要求。

另一种想法是，先预存语音片段，在合成语音时根据算法选择相应的语音片段，再通过拼接算法在时域上进行拼接。但是这样得到的合成语音受到语音库中语音风格的限制，难以变化；语音合成的好坏直接取决于语料库的规模大学。直到1989 年，E.Moulines 和F.Charpentier
提出了一种可以调节浊音基频和时长的基音同步叠加算法（Pitch Synchronous Overlap and Add，PSOLA），由此可以获得有较高清晰度和自然度的合成语音。进入90年代后，存储空间不再受限，日本科学家提出一种多样本不等长波形拼接合成技术\cite{ref4}，语音库中存放了音素、音节等基本拼接基元以及词、短语和句子等扩展拼接基元。逐渐发展成基于样本调整和大语料库单元的单元挑选拼接技术。汉语音节独立性较强，音节的音段特征比较稳定，但音高、音长和音强等韵律特征在连续语流中变化复杂，这些韵律特征又直接影响了汉语语音合成的自然度，因此很适合采用基于PSOLA 方法的波形拼接法来合成\cite{ref3}。 在国内，中科院声学所和清华大学等都基于此设计了汉语文语转换系统；科大讯飞等公司也推出了具商业实用程度的文语转换系统。

通过上述介绍，我们可以看到基于参数的合成方法灵活度高，但准确提取参数困难，合成音质受参数的影响自然度不高；基于波形拼接的合成方法自然度高，但这依赖于规模庞大的数据库，可扩展性不好。随着机器学习在各个邻域的发展应用以及声码器技术的进步，在90年代末期，发展出了另一种数据驱动的方法“统计参数语音合成”。其中最主流的就是基于隐形马尔可夫模型的语音合成方法。

\section{当前业界技术}
\subsection{基于HMM的语音合成技术}
通过上述介绍，我们可以看到基于参数的合成方法灵活度高，但准确提取参数困难，合成音质受参数的影响自然度不高；基于波形拼接的合成方法自然度高，但这依赖于规模庞大的数据库，可扩展性不好。随着机器学习在各个邻域的发展应用以及声码器技术的进步，在90年代末期，发展出了另一种数据驱动的方法“统计参数语音合成”。其中最主流的就是基于隐形马尔可夫模型的语音合成方法\cite{ref5}。

传统上，一个完整的语音合成系统包括前端（front-end）的文本分析阶段，语言学特征到声学特征的声学建模阶段和声码器（vocoder）由声学特征产生声音波形从而发声的合成语音阶段。在前端文本分析阶段涉及到自然语言处理，一般需要语言学领域的专业知识。分词；进行文本规范化，比如确认单词、句子的结束，对空格、标点、日期和缩写等的处理；语素转换成音素并对齐；标注重音、发音边界和音长等。而在基于参数的语音合成中，“参数”即一系列随时间变化的声学特征，比如共振峰模型中每个共振峰的频率。在基于HMM的语音合成模型中是通过STRAIGHT语音分析合成器提取出的基频（F0）和谱参数（比如梅尔倒谱：MFCC）等，因此在声学建模中需要对序列数据建模，在语音识别邻域中HMM已经有了广泛的应用。

语音库中有预先录入的语音和对应的文本，对文本分析得到标注的语言学特征，由STRAIGHT提取语音中的声学特征，二者经过对齐就得到了训练数据，在训练过程中首先训练单音素的HMM模型，然后由单音素模型进过上下文相关的HMM训练得到上下文相关的模型。又由于音素组合太多，为减轻过拟合和训练数据不足造成的影响，根据特定语种设计上下文属性问题集，对HMM模型聚类（比如基于决策树的聚类算法），然后对时长进行聚类建模，最终得到声学模型。在合成语音的时候，同样先对输入文本进行文本分析，得到有上下文信息标注的语言学特征，将相应的HMM模型连接起来，然后通过模型预测出声学特征，由声学特征合成语音。

基于HMM的语音合成技术还有多种变形和改进，比如考虑到实际声音存在连续性，随时间增长，同一状态出现的概率成指数衰减，因此采用基于半马尔可夫模型（HSMM）。此外，还可以将基于HMM的统计参数模型和波形拼接方法相结合，也有很好的合成效果。
\subsection{基于深度学习的语音合成}
在统计参数语音合成中，限制了合成语音自然度的一个重要因素在于声学建模的准确性。学习语言学特征和声学特征的关系实际上是一个复杂的非线性回归问题，所以需要比HMM更有力的回归模型，比如神经网络。早在90年代，就有利用神经网络来学习语言学特征和声学特征的关系的研究\cite{ref7}，或者在语音合成系统的各个模块中应用神经网络，比如从原始输入文本中提取语言学特征\cite{ref8}，预测时长。发展到现在，有了更多的训练数据，更好的计算资源，更先进的训练算法，一个完整语音合成系统中声码器等技术也有很大的改进，模型中的隐藏层更多。比如用限制玻尔兹曼机（RBMs）来代替混合高斯模型来对声学特征的分布建模；为了包括上下文信息，采用基于LSTM或者GRU（gated recurrent unit）的循环神经网络直接学习语言学特征和声学特征之间的关系\cite{ref9}。

关于用深度神经网络取代TTS系统中的各个模块的研究有很多，包括语素转音素模型\cite{ref10}，音素时长预测模型\cite{ref11}，基频预测模型\cite{ref12}和语音合成模型\cite{ref13}等。下面我们介绍一些代表性的成果。

%\begin{enumerate}[1)]
\textbf{1) WaveNet\cite{ref14}}

之前的语音合成会提取出声码器需要的声学特征然后进过声码器合成语音，这限制了语音合成的自然度，DeepMind在2016年受到PixelRNN的启发，直接对声音波形建模。它将波形中一些列采样点的产生建模成一个自回归模型，即每一个采样点由它之前所有的采样点预测得到，则波形$\textbf{x}=\{x_1,\cdots,x_T\}$的联合概率分布就简化为：
\begin{equation}\label{equ:autoregressice}
p(\textbf{x})=\prod\limits^{T}_{t=1}p(x_t|x_1,\cdots,x_{t-1})
\end{equation}

和PixelCNNs类似，WaveNet中采用堆叠的卷积神经网络来生成上述条件概率分布。主要组成部分是因果卷积网络，如图\ref{img:causal}，它保证了在第t时间步的输出只由它之前的时间步影响。在训练阶段，由于\textbf{x}的真实值是已知的，所以卷积可以并行地进行，所以它比RNN训练要快，但是保证高分辨率的采样点序列对时间的依赖跨度长，需要增大感受野（receptive field），这就需要在因果卷积中用规模更大的卷积核或者更深的卷积层，而这会使计算量迅速增加。为了减少计算量，使用空洞卷积的技巧。最终输出和输入具有相同的时间维数，并且用softmax分布来对每一个采样点的条件概率建模，论文中认为它比混合模型的效果更好。[gated activation units]，此外，为了加快收敛和训练更深的模型在整个网络中还用到了残差和跳步连接。

和SampleRnn相比，WaveNet的一大优势在于它可以在输入时提供额外的特征\textbf{h}，则此时对$p(\textbf{x}|\textbf{h})$建模有：
\begin{equation}\label{equ:conditioned}
p(\textbf{x}|\textbf{h})=\prod\limits_{t=1}^{T}p(x_t|x_1,\cdots,x_{t-1},\textbf{h})
\end{equation}
由此可以产生具有不同特点的音频，比如用来产生不同说话人语音，生成音乐。对于TTS来说，通过在WaveNet的每一层额外输入语言学特征和$\log F_0$ （它们预先同一个额外的transposed CNN作为上采样器，以上采样到和采样点相同的分辨率）生成对应的语音，在论文中以MOS （mean opinion score）作为评价标准，WaveNet 的表现优于基于LSTM-RNN和基于HMM 的拼接合成方法。

但是WaveNet有一个明显的缺点，在合成阶段它需要一个采样点一个采样点地依次预测，为了达到好的保真度，按16Hz的采样率，合成一秒的音频也需要很长时间，不实用。不过，其后发展出了WaveNet的并行版本，大大提升了合成速率。

\textbf{2) Deep Voice1,2\cite{ref17,ref18}}

2017年，百度受到传统TTS流程的启发，提出了Deep Voice，采用相同的结构，但是每一部分都是一个神经网络，这样得到的TTS系统更简单灵活。本来每一部分都需要大量特征工程和专业知识，Deep Voice减少了这些要求。之前需要手动提取谱包络、频谱参数和非周期参数等特征，Deep Voice中只需要带重音标注的音素，音素时长和基频（$F_0$），而这些特征都可以通过神经网络学得。Deep Voice 包括以下几个部分：
\begin{enumerate}
\item 语素转音素模型
在这一部分，可以通过已有的音素词典得到文本对应的音素序列，但是为了使词典中没有的语素也能有音素表示，基于编码-解码（Encoder-Decoder）结构进行语素序列到音素序列的seq2seq学习，训练一个语素转音素的神经网络模型。
\item 分割模型（segmentation model）
这一部分只在训练过程中作用，基于百度语音识别deep speech2中的对齐技术，确定音素在音频中对应的起止位置，以得到音素时长好在下一阶段的时长模型中作为标注使用。
\item 音素时长和基频预测的多任务模型
在这一部分用一个模型同时预测音素时长和基频。输入是编码成one-hot向量的带重音的音素，输出时长和音素是否发音的频率以及基频。训练时的目标是最小化一个联合损失（包括时长误差，$F_0$误差，发音概率的负对数似然和用于保证光滑性的$F_0$随时间变化的惩罚）。
\item 音频合成模型
在这一部分里，Deep Voice改进了WaveNet，得到了更快的合成速度。
\end{enumerate}

在随后的Deep Voice2中，它首先对上述Deep Voice的各个模块进行了改进，时长预测和基频预测模型分开，在时长模型中将问题转化成一个序列标注问题，然后将预测出的音素时长从每个音素的特征上采样成每一帧的特征，传入基频预测模型。这些都使得它比相比于Deep Voice1有明显的音频质量改进。

另一方面，Deep Voice2通过增加低维可训练的说话人词向量嵌入，从单个语音合成模型中就能产生不同的说话人声音。在之前的多说话人TTS中，基于HMM 的TTS 采用一个平均声音模型，基于DNN的TTS通常采用一个固定的i-vector来表示说话人，然后每一个说话人有分开的输出层。在Deep Voice2中，针对Deep Voice 和Tacotron(见后文)提出了利用可训练的说话人嵌入向量来训练模型。对Deep Voice，在每一部分都添加说话人嵌入向量；对Tacotron，在Encoder、Decoder和Vocoder阶段都加入说话人信息。

\textbf{3) Tacotron\cite{ref16}}

前面已经介绍过传统的TTS流程很复杂，通常有一个文本处理前端需要提取多种语言学特征，一个时长预测模型和声学特征预测模型及一个基于信号处理的声码器。每个模块都需要大量专业知识，设计复杂；而且各模块独立训练，产生的错误会累积。因此一个很自然的发展方向是将各个阶段融合在一起，直接从文本输入合成语音，一个端到端的模型有很多好处，比如可以减少特征工程的需求，能方便地从文本输入阶段就引入新的特征。

Tacotron 是Google 在2017 年提出的一个基于注意力机制的seq2seq 的端到端语音合成模型，它能直接从输入的字符序列得到频谱特征。和Wang等\cite{ref17}以及Char2Wav的端到端TTS 相比，Tacotron 的端到端程度更高，Wang等需要一个HMM 模型来学习对齐并且预测的是声码器特征，Char2Wav 虽然可以直接在字符上训练，但预测的仍是声码器的声学特征。

Tacotron的主体就是引入注意力机制的seq2seq模型。将字符序列转化成嵌入向量，作为编码器（Encoder）的输入，在预处理网络中进行一系列非线性变换，并通过一个CBHG模块（1-D convolution bank + highway network + bidirectional GRU）得到输入序列的特征表达。在解码器（Decoder）中，应用基于注意力机制的RNN，每一给RNN单元都一次性预测出r帧的频谱特征，比如Linear-scale spectrum或者Mel-scale spectrum，只要这些特征能提供足够的声音信息即可。最后经过一个后处理网络将解码器中预测出的序列转化成合成声音波形所需的序列，并且此时它可以利用利用整个序列的信息，而不受从左到右的顺序限制，随后语音合成器合成语音，论文中用Griffin-Lim 作为合成器。

很显然，在上述过程中，Griffin-Lim作为合成器可以被替换成WaveNet以获得更好的合成效果。在Tacotron2中，大体流程和Tacotron类似，结构更精简，在编码器和解码器中都只用了最基础的卷积层和LSTM。最终预测出梅尔频谱，然后通过WaveNet合成音频。

\textbf{4) Deep Voice3}

Deep Voice3和百度之前提出的Deep Voice1,2结构完全不同，它是一个基于注意力机制的全卷积的语音合成系统。它和Char2Wav以及Tacotron不同，避免了使用RNN，全卷积的结构使得它可以并行计算，加快了速度，并且减少了一般注意力模型可能错误的发生。
\begin{enumerate}
\item Encoder: 全卷积网络，将文本特征转化成中间表示
\item Decoder：因果卷积，将学到的中间表示以自回归的方式解码成一个低维的音频表示，比如梅尔频谱。
\item Converter: 一个全卷积的后处理网络，根据最后用的声波合成器的不同（Griffin-lim、WaveNet、WORLD等），从解码器的隐藏状态预测出最终输出特征。在这一部分中并没有用到因果卷积，所以可以充分利用上下文信息。
\end{enumerate}
和Deep Voice2一样，在Encoder、Decoder和Converter中添加说话人信息，可以得到多说话人语音合成模型。

[monotonic attention behavior]

\textbf{5) ClariNet}
%\end{enumerate}

\section{国内企业现状}
\section{相关开源项目}

[1] 韩纪庆，语音信号处理
\end{document}
